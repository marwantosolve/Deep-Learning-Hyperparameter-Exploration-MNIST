{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6409ebc7",
   "metadata": {},
   "source": [
    "# Deep-Learning-Hyperparameter-Exploration-MNIST Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d58f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 1: Setup & Baseline Models\n",
    "- Set up shared Google Colab notebook\n",
    "- Load MNIST dataset (shape 28x28)\n",
    "- Shuffle dataset\n",
    "- Implement basic ANN\n",
    "- Implement SVM\n",
    "- Document accuracy, layers, params, train/test time for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9926f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for ANN\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train_ann = x_train.astype('float32') / 255.0\n",
    "x_test_ann = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_ann = to_categorical(y_train, num_classes=10)\n",
    "y_test_ann = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "\"\"\"\n",
    "NOte Important Fsh5:\n",
    "Shuffle dataset (Keras does this by default during training if shuffle=True)\n",
    "For SVM, we'll use the original shapes and shuffle manually if needed,\n",
    "but scikit-learn's train_test_split or model's shuffle usually handles this.\n",
    "For simplicity here, we'll use the already split data\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Step 1: A) Basic ANN ---\")\n",
    "# Implement basic ANN\n",
    "ann_model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images\n",
    "    Dense(128, activation='relu'),   # A hidden layer with 128 neurons\n",
    "    # Output layer with 10 neurons for 10 classes\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "ann_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "ann_model.summary()\n",
    "\n",
    "# Train ANN\n",
    "start_time_ann_train = time.time()\n",
    "history_ann = ann_model.fit(\n",
    "    x_train_ann, y_train_ann, epochs=5, batch_size=32, verbose=1, validation_split=0.1)\n",
    "end_time_ann_train = time.time()\n",
    "ann_train_time = end_time_ann_train - \n",
    "\n",
    "# Evaluate ANN\n",
    "start_time_ann_test = time.time()\n",
    "loss_ann, accuracy_ann = ann_model.evaluate(x_test_ann, y_test_ann, verbose=0)\n",
    "end_time_ann_test = time.time()\n",
    "ann_test_time = end_time_ann_test - start_time_ann_test\n",
    "\n",
    "print(f\"ANN - Accuracy: {accuracy_ann*100:.2f}%\")\n",
    "print(f\"ANN - Layers: {len(ann_model.layers)}\")\n",
    "print(f\"ANN - Parameters: {ann_model.count_params()}\")\n",
    "print(f\"ANN - Training Time: {ann_train_time:.2f} seconds\")\n",
    "print(f\"ANN - Test Time: {ann_test_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b560f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for SVM\n",
    "\n",
    "# Flatten images\n",
    "x_train_svm = x_train.reshape(x_train.shape[0], -1).astype('float32') / 255.0\n",
    "x_test_svm = x_test.reshape(x_test.shape[0], -1).astype('float32') / 255.0\n",
    "# For SVM, we use original y_train, y_test (not one-hot encoded)\n",
    "\n",
    "# Due to SVM's computational complexity, let's use a subset of data for faster training\n",
    "subset_size = 10000  # Using 10k samples for training, 2k for testing\n",
    "x_train_svm_subset = x_train_svm[:subset_size]\n",
    "y_train_svm_subset = y_train[:subset_size]\n",
    "x_test_svm_subset = x_test_svm[:subset_size // 5]\n",
    "y_test_svm_subset = y_test[:subset_size // 5]\n",
    "\n",
    "print(\"\\n--- Step 1: B) SVM ---\")\n",
    "# Implement SVM\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale',\n",
    "                random_state=42)  # Common defaults\n",
    "\n",
    "# Train SVM\n",
    "start_time_svm_train = time.time()\n",
    "svm_model.fit(x_train_svm_subset, y_train_svm_subset)\n",
    "end_time_svm_train = time.time()\n",
    "svm_train_time = end_time_svm_train - start_time_svm_train\n",
    "\n",
    "# Evaluate SVM\n",
    "start_time_svm_test = time.time()\n",
    "y_pred_svm = svm_model.predict(x_test_svm_subset)\n",
    "accuracy_svm = accuracy_score(y_test_svm_subset, y_pred_svm)\n",
    "end_time_svm_test = time.time()\n",
    "svm_test_time = end_time_svm_test - start_time_svm_test\n",
    "\n",
    "print(f\"SVM - Accuracy (on subset): {accuracy_svm*100:.2f}%\")\n",
    "# SVM layers are not typically counted like NNs. We can note the kernel and type.\n",
    "print(f\"SVM - Model Type: SVC with RBF kernel\")\n",
    "# SVM parameters are not directly comparable to NN params in the same way.\n",
    "# n_support_ is the number of support vectors.\n",
    "print(f\"SVM - Number of support vectors: {np.sum(svm_model.n_support_)}\")\n",
    "print(f\"SVM - Training Time (on subset): {svm_train_time:.2f} seconds\")\n",
    "print(f\"SVM - Test Time (on subset): {svm_test_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed53291",
   "metadata": {},
   "source": [
    "## Step 2: Base CNN + Epoch Tuning\n",
    "- Implement CNN with 3 layers (include Conv2D + MaxPooling2D)\n",
    "- Use ReLU, SGD, no dropout\n",
    "- Test different epoch values (10â€“25)\n",
    "- Log results: accuracy, params, time, layers\n",
    "- Choose best epoch count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e69194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for CNN (needs channel dimension)\n",
    "x_train_cnn = x_train_ann.reshape(x_train_ann.shape[0], 28, 28, 1)\n",
    "x_test_cnn = x_test_ann.reshape(x_test_ann.shape[0], 28, 28, 1)\n",
    "\n",
    "print(\"\\n--- Step 2: A) Base CNN ---\")\n",
    "def create_base_cnn_model():\n",
    "  model = Sequential([\n",
    "      Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "      MaxPooling2D((2, 2)),\n",
    "      Conv2D(64, (3, 3), activation='relu'),\n",
    "      MaxPooling2D((2, 2)),\n",
    "      Conv2D(64, (3, 3), activation='relu'),\n",
    "      Flatten(),\n",
    "      Dense(10, activation='softmax')\n",
    "  ])\n",
    "  # Using SGD as specified\n",
    "  optimizer = SGD(learning_rate=0.01)  # A common default LR for SGD\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 2: B) + Epoch Tuning ---\")\n",
    "\n",
    "epoch_options = [10, 15, 20, 25]\n",
    "best_epoch_cnn = -1\n",
    "best_accuracy_cnn = 0\n",
    "best_history_cnn = None\n",
    "results_cnn_epoch_tuning = []\n",
    "\n",
    "print(\"\\nTesting different epoch values for Base CNN:\")\n",
    "for epochs in epoch_options:\n",
    "  print(f\"\\nTraining Base CNN with {epochs} epochs...\")\n",
    "  cnn_model = create_base_cnn_model()\n",
    "  if epochs == epoch_options[0]:  # Print summary only once\n",
    "    cnn_model.summary()\n",
    "\n",
    "  start_time_cnn_train = time.time()\n",
    "  history = cnn_model.fit(x_train_cnn, y_train_ann,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=32,\n",
    "                          verbose=1,\n",
    "                          # Evaluate on test set for simplicity here\n",
    "                          validation_data=(x_test_cnn, y_test_ann)\n",
    "                          )\n",
    "  end_time_cnn_train = time.time()\n",
    "  cnn_train_time = end_time_cnn_train - start_time_cnn_train\n",
    "\n",
    "  start_time_cnn_test = time.time()\n",
    "  # Evaluation is done as part of validation_data in fit, or can be done separately:\n",
    "  loss_cnn, accuracy_cnn = cnn_model.evaluate(\n",
    "      x_test_cnn, y_test_ann, verbose=0)\n",
    "  end_time_cnn_test = time.time()\n",
    "  cnn_test_time = end_time_cnn_test - start_time_cnn_test\n",
    "\n",
    "  print(f\"CNN ({epochs} epochs) - Test Accuracy: {accuracy_cnn*100:.2f}%\")\n",
    "  print(f\"CNN ({epochs} epochs) - Layers: {len(cnn_model.layers)}\")\n",
    "  print(f\"CNN ({epochs} epochs) - Parameters: {cnn_model.count_params()}\")\n",
    "  print(f\"CNN ({epochs} epochs) - Training Time: {cnn_train_time:.2f} seconds\")\n",
    "  print(f\"CNN ({epochs} epochs) - Test Time: {cnn_test_time:.2f} seconds\")\n",
    "\n",
    "  results_cnn_epoch_tuning.append({\n",
    "      'epochs': epochs,\n",
    "      'accuracy': accuracy_cnn,\n",
    "      'params': cnn_model.count_params(),\n",
    "      'layers': len(cnn_model.layers),\n",
    "      'train_time': cnn_train_time,\n",
    "      'test_time': cnn_test_time\n",
    "  })\n",
    "\n",
    "  if accuracy_cnn > best_accuracy_cnn:\n",
    "    best_accuracy_cnn = accuracy_cnn\n",
    "    best_epoch_cnn = epochs\n",
    "    # Potentially save the model or its weights if needed later\n",
    "    # best_history_cnn = history # Store history if detailed analysis is needed\n",
    "\n",
    "print(\n",
    "    f\"\\nBest epoch count for Base CNN: {best_epoch_cnn} with accuracy: {best_accuracy_cnn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a9344",
   "metadata": {},
   "source": [
    "## Step 3: Learning Rate Testing\n",
    "- Fix best epoch from Step 2\n",
    "- Test learning rates (e.g., 0.01, 0.001, 0.0001)\n",
    "- Document impact on performance\n",
    "- Select optimal LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce002f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 3: Learning Rate Testing ---\")\n",
    "\n",
    "# Use the best_epoch_cnn from Step 2\n",
    "fixed_epochs_for_lr_testing = best_epoch_cnn\n",
    "print(f\"Using fixed epoch count: {fixed_epochs_for_lr_testing}\")\n",
    "\n",
    "learning_rate_options = [0.01, 0.001, 0.0001]  # As suggested\n",
    "best_lr_cnn = -1\n",
    "best_accuracy_lr_cnn = 0\n",
    "results_cnn_lr_tuning = []\n",
    "\n",
    "\n",
    "def create_cnn_model_for_lr_testing(learning_rate):\n",
    "  model = Sequential([\n",
    "      Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "      MaxPooling2D((2, 2)),\n",
    "      Conv2D(64, (3, 3), activation='relu'),\n",
    "      MaxPooling2D((2, 2)),\n",
    "      Conv2D(64, (3, 3), activation='relu'),\n",
    "      Flatten(),\n",
    "      Dense(10, activation='softmax')\n",
    "  ])\n",
    "  optimizer = SGD(learning_rate=learning_rate)\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "print(\"\\nTesting different learning rates for Base CNN:\")\n",
    "for lr in learning_rate_options:\n",
    "  print(\n",
    "      f\"\\nTraining Base CNN with LR={lr} for {fixed_epochs_for_lr_testing} epochs...\")\n",
    "  cnn_lr_model = create_cnn_model_for_lr_testing(learning_rate=lr)\n",
    "  # No need to print summary repeatedly, structure is same as base CNN\n",
    "\n",
    "  start_time_cnn_lr_train = time.time()\n",
    "  history_lr = cnn_lr_model.fit(x_train_cnn, y_train_ann,\n",
    "                                epochs=fixed_epochs_for_lr_testing,\n",
    "                                batch_size=32,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_test_cnn, y_test_ann)\n",
    "                                )\n",
    "  end_time_cnn_lr_train = time.time()\n",
    "  cnn_lr_train_time = end_time_cnn_lr_train - start_time_cnn_lr_train\n",
    "\n",
    "  loss_cnn_lr, accuracy_cnn_lr = cnn_lr_model.evaluate(\n",
    "      x_test_cnn, y_test_ann, verbose=0)\n",
    "\n",
    "  print(f\"CNN (LR={lr}) - Test Accuracy: {accuracy_cnn_lr*100:.2f}%\")\n",
    "  print(f\"CNN (LR={lr}) - Training Time: {cnn_lr_train_time:.2f} seconds\")\n",
    "\n",
    "  results_cnn_lr_tuning.append({\n",
    "      'learning_rate': lr,\n",
    "      'accuracy': accuracy_cnn_lr,\n",
    "      'train_time': cnn_lr_train_time,\n",
    "      # Params and layers are same as base CNN, so not logged again unless model changes\n",
    "  })\n",
    "\n",
    "  if accuracy_cnn_lr > best_accuracy_lr_cnn:\n",
    "    best_accuracy_lr_cnn = accuracy_cnn_lr\n",
    "    best_lr_cnn = lr\n",
    "\n",
    "print(\n",
    "    f\"\\nBest learning rate for Base CNN: {best_lr_cnn} with accuracy: {best_accuracy_lr_cnn*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60309c4",
   "metadata": {},
   "source": [
    "## Step 4: Model Architecture Variation\n",
    "- Test models with different Conv/FC layers (3 Conv max, 4 FC max)\n",
    "- Aim for high accuracy with fewer params\n",
    "- Document 4+ different configs\n",
    "- Capture full Segment B data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 4: Model Architecture Variation ---\")\n",
    "\n",
    "# Use best settings from Step 2 and 3\n",
    "fixed_epochs_arch = best_epoch_cnn\n",
    "fixed_lr_arch = best_lr_cnn\n",
    "print(f\"Using fixed epochs: {fixed_epochs_arch}, fixed LR: {fixed_lr_arch}\")\n",
    "\n",
    "# Store results for architecture variations\n",
    "arch_variation_results = []\n",
    "\n",
    "\n",
    "def create_variable_cnn_model(conv_layers_config, fc_layers_config, learning_rate):\n",
    "  \"\"\"\n",
    "  Creates a CNN model based on specified convolutional and fully connected layer configurations.\n",
    "  conv_layers_config: list of tuples, e.g., [(filters, kernel_size), ...]\n",
    "                      Kernel size should be (size, size)\n",
    "  fc_layers_config: list of integers, e.g., [neurons_fc1, neurons_fc2, ...]\n",
    "                      Each <= 512 as per constraints.\n",
    "  \"\"\"\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(conv_layers_config[0][0], conv_layers_config[0]\n",
    "            [1], activation='relu', input_shape=(28, 28, 1)))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "  for i in range(1, len(conv_layers_config)):\n",
    "    if i < 3:  # Max 3 Conv layers including the first one implies 2 more here\n",
    "      model.add(\n",
    "          Conv2D(conv_layers_config[i][0], conv_layers_config[i][1], activation='relu'))\n",
    "      model.add(MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "      # If more conv_layers are specified than allowed, we might add them without pooling or break\n",
    "      # For now, sticking to max 3 conv blocks with pooling\n",
    "      print(f\"Warning: Exceeding 3 Conv layers with pooling. Additional Conv layers ignored in this setup.\")\n",
    "      break\n",
    "\n",
    "  model.add(Flatten())\n",
    "\n",
    "  for i in range(len(fc_layers_config)):\n",
    "    if i < 4:  # Max 4 FC layers (excluding output layer, or including it?)\n",
    "        # Assuming this constraint is for hidden FC layers.\n",
    "      neurons = fc_layers_config[i]\n",
    "      if neurons > 512:\n",
    "        print(\n",
    "            f\"Warning: FC layer {i+1} with {neurons} neurons exceeds 512. Capping to 512.\")\n",
    "        neurons = 512\n",
    "      model.add(Dense(neurons, activation='relu'))\n",
    "    else:\n",
    "      print(f\"Warning: Exceeding 4 FC layers. Additional FC layers ignored.\")\n",
    "      break\n",
    "\n",
    "  model.add(Dense(10, activation='softmax'))  # Output layer\n",
    "\n",
    "  optimizer = SGD(learning_rate=learning_rate)\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining some architectures to test\n",
    "Config format: (conv_layer_specs, fc_layer_specs)\n",
    "conv_layer_spec: list of (filters, (kernel_x, kernel_y))\n",
    "fc_layer_spec: list of (neuron_count)\n",
    "\"\"\"\n",
    "\n",
    "architectures_to_test = [\n",
    "    # Config 1: Base model (3 Conv, 0 hidden FC - already tested, but good baseline here)\n",
    "    {'name': '3Conv_0HiddenFC',\n",
    "     'conv': [(32, (3, 3)), (64, (3, 3)), (64, (3, 3))],\n",
    "     'fc': []},\n",
    "    # Config 2: 2 Conv, 1 Hidden FC (128 neurons)\n",
    "    {'name': '2Conv_1HiddenFC_128',\n",
    "     'conv': [(32, (3, 3)), (64, (3, 3))],\n",
    "     'fc': [128]},\n",
    "    # Config 3: 3 Conv, 1 Hidden FC (64 neurons) - fewer params than base + FC\n",
    "    {'name': '3Conv_1HiddenFC_64',\n",
    "     'conv': [(32, (3, 3)), (64, (3, 3)), (64, (3, 3))],\n",
    "     'fc': [64]},\n",
    "    # Config 4: 2 Conv, 2 Hidden FC (128, 64 neurons)\n",
    "    {'name': '2Conv_2HiddenFC_128_64',\n",
    "     'conv': [(32, (5, 5)), (64, (5, 5))],  # Using 5x5 kernels here\n",
    "     'fc': [128, 64]},\n",
    "    # Config 5: 1 Conv, 1 Hidden FC (256 neurons) - fewer conv, more FC\n",
    "    {'name': '1Conv_1HiddenFC_256',\n",
    "     'conv': [(64, (5, 5))],\n",
    "     'fc': [256]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTesting different CNN architectures:\")\n",
    "for i, arch_config in enumerate(architectures_to_test):\n",
    "  print(f\"\\n--- Architecture {i+1}: {arch_config['name']} ---\")\n",
    "\n",
    "  # Ensure kernel sizes are valid tuples\n",
    "  current_conv_config = []\n",
    "  for filters, kernel_dim in arch_config['conv']:\n",
    "    if isinstance(kernel_dim, int):\n",
    "      kernel_tuple = (kernel_dim, kernel_dim)\n",
    "    else:\n",
    "      kernel_tuple = kernel_dim\n",
    "    if kernel_tuple[0] > 5 or kernel_tuple[1] > 5:\n",
    "      print(\n",
    "          f\"Warning: Kernel {kernel_tuple} for {arch_config['name']} exceeds 5x5. Skipping this config or capping kernel.\")\n",
    "      # For simplicity, we might skip or cap here. Capping:\n",
    "      kernel_tuple = (min(kernel_tuple[0], 5), min(kernel_tuple[1], 5))\n",
    "      print(f\"Capped kernel to: {kernel_tuple}\")\n",
    "    current_conv_config.append((filters, kernel_tuple))\n",
    "\n",
    "  arch_model = create_variable_cnn_model(current_conv_config,\n",
    "                                         arch_config['fc'],\n",
    "                                         learning_rate=fixed_lr_arch)\n",
    "  arch_model.summary()\n",
    "\n",
    "  start_time_arch_train = time.time()\n",
    "  history_arch = arch_model.fit(x_train_cnn, y_train_ann,\n",
    "                                epochs=fixed_epochs_arch,\n",
    "                                batch_size=32,\n",
    "                                verbose=1,\n",
    "                                validation_data=(x_test_cnn, y_test_ann)\n",
    "                                )\n",
    "  end_time_arch_train = time.time()\n",
    "  arch_train_time = end_time_arch_train - start_time_arch_train\n",
    "\n",
    "  loss_arch, accuracy_arch = arch_model.evaluate(\n",
    "      x_test_cnn, y_test_ann, verbose=0)\n",
    "\n",
    "  print(\n",
    "      f\"Architecture {arch_config['name']} - Test Accuracy: {accuracy_arch*100:.2f}%\")\n",
    "  print(\n",
    "      f\"Architecture {arch_config['name']} - Parameters: {arch_model.count_params()}\")\n",
    "  print(\n",
    "      f\"Architecture {arch_config['name']} - Training Time: {arch_train_time:.2f} seconds\")\n",
    "\n",
    "  arch_variation_results.append({\n",
    "      'name': arch_config['name'],\n",
    "      'accuracy': accuracy_arch,\n",
    "      'params': arch_model.count_params(),\n",
    "      'conv_config': current_conv_config,\n",
    "      'fc_config': arch_config['fc'],\n",
    "      'train_time': arch_train_time,\n",
    "      'layers': len(arch_model.layers)  # Number of functional layers\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best architecture based on accuracy and params (e.g. accuracy/params ratio or a preferred metric)\n",
    "# For now, just printing all results. User can analyze this list.\n",
    "print(\"\\n--- Architecture Variation Results ---\")\n",
    "for res in arch_variation_results:\n",
    "  print(f\"Name: {res['name']}, Accuracy: {res['accuracy']*100:.2f}%, Params: {res['params']}, Train Time: {res['train_time']:.2f}s\")\n",
    "\n",
    "  # Storing the best architecture found so far for next steps\n",
    "# This simple selection prefers highest accuracy, then lowest params\n",
    "best_overall_arch_config = None\n",
    "best_overall_arch_accuracy = 0\n",
    "best_overall_arch_params = float('inf')\n",
    "\n",
    "for res in arch_variation_results:\n",
    "  if res['accuracy'] > best_overall_arch_accuracy:\n",
    "    best_overall_arch_accuracy = res['accuracy']\n",
    "    best_overall_arch_params = res['params']\n",
    "    best_overall_arch_config = res\n",
    "  elif res['accuracy'] == best_overall_arch_accuracy:\n",
    "    if res['params'] < best_overall_arch_params:\n",
    "      best_overall_arch_params = res['params']\n",
    "      best_overall_arch_config = res\n",
    "\n",
    "if best_overall_arch_config:\n",
    "  print(\n",
    "      f\"\\nSelected best architecture for Step 5: {best_overall_arch_config['name']} with Acc: {best_overall_arch_config['accuracy']*100:.2f}%, Params: {best_overall_arch_config['params']}\")\n",
    "  # These will be used by step 5\n",
    "  chosen_conv_config = best_overall_arch_config['conv_config']\n",
    "  chosen_fc_config = best_overall_arch_config['fc_config']\n",
    "else:\n",
    "  print(\"\\nNo architecture configurations were successfully tested or no results available.\")\n",
    "  # Fallback to base model config if none better or tested\n",
    "  chosen_conv_config = [(32, (3, 3)), (64, (3, 3)), (64, (3, 3))]\n",
    "  chosen_fc_config = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776fc8d",
   "metadata": {},
   "source": [
    "## Step 5: Batch Sizes & Activations\n",
    "- Try 2 new batch sizes (e.g., 128, 192)\n",
    "- Test 3 new activations (include sigmoid)\n",
    "- Compare performance metrics\n",
    "- Finalize best batch size and activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc13c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 5: Batch Sizes & Activations ---\")\n",
    "\n",
    "# Use best settings from previous steps\n",
    "# chosen_conv_config, chosen_fc_config, fixed_lr_arch, fixed_epochs_arch are available\n",
    "print(\n",
    "    f\"Using best arch: Conv-{chosen_conv_config}, FC-{chosen_fc_config}, LR-{fixed_lr_arch}, Epochs-{fixed_epochs_arch}\")\n",
    "\n",
    "# Current (32) and two new ones, all <= 250\n",
    "batch_size_options = [32, 128, 192]\n",
    "best_batch_size = -1\n",
    "best_accuracy_batch = 0\n",
    "results_batch_tuning = []\n",
    "\n",
    "# Modify create_variable_cnn_model to accept activation function\n",
    "def create_cnn_model_for_step5(conv_layers_config, fc_layers_config, learning_rate, activation_func):\n",
    "  model = Sequential()\n",
    "  # First Conv Layer (with input_shape)\n",
    "  model.add(Conv2D(conv_layers_config[0][0], conv_layers_config[0]\n",
    "            [1], activation=activation_func, input_shape=(28, 28, 1)))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "  # Subsequent Conv Layers\n",
    "  for i in range(1, len(conv_layers_config)):\n",
    "    model.add(Conv2D(\n",
    "        conv_layers_config[i][0], conv_layers_config[i][1], activation=activation_func))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "  model.add(Flatten())\n",
    "\n",
    "  # FC Layers\n",
    "  for neurons in fc_layers_config:\n",
    "    model.add(Dense(neurons, activation=activation_func))\n",
    "\n",
    "  model.add(Dense(10, activation='softmax'))  # Output layer\n",
    "\n",
    "  optimizer = SGD(learning_rate=learning_rate)\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "print(\"\\nTesting different batch sizes:\")\n",
    "# Test with 'relu' activation first to find best batch size\n",
    "fixed_activation_for_batch_test = 'relu'\n",
    "\n",
    "for batch_s in batch_size_options:\n",
    "  print(\n",
    "      f\"\\nTraining with Batch Size: {batch_s}, Activation: {fixed_activation_for_batch_test}...\")\n",
    "  model_bs = create_cnn_model_for_step5(chosen_conv_config,\n",
    "                                        chosen_fc_config,\n",
    "                                        fixed_lr_arch,\n",
    "                                        fixed_activation_for_batch_test)\n",
    "  # No need for summary if architecture isn't changing here, just batch size\n",
    "\n",
    "  start_time_bs_train = time.time()\n",
    "  history_bs = model_bs.fit(x_train_cnn, y_train_ann,\n",
    "                            epochs=fixed_epochs_arch,\n",
    "                            batch_size=batch_s,\n",
    "                            verbose=1,\n",
    "                            validation_data=(x_test_cnn, y_test_ann)\n",
    "                            )\n",
    "  end_time_bs_train = time.time()\n",
    "  bs_train_time = end_time_bs_train - start_time_bs_train\n",
    "\n",
    "  loss_bs, accuracy_bs = model_bs.evaluate(x_test_cnn, y_test_ann, verbose=0)\n",
    "\n",
    "  print(f\"Batch Size {batch_s} - Test Accuracy: {accuracy_bs*100:.2f}%\")\n",
    "  print(f\"Batch Size {batch_s} - Training Time: {bs_train_time:.2f} seconds\")\n",
    "\n",
    "  results_batch_tuning.append({\n",
    "      'batch_size': batch_s,\n",
    "      'accuracy': accuracy_bs,\n",
    "      'train_time': bs_train_time\n",
    "  })\n",
    "\n",
    "  if accuracy_bs > best_accuracy_batch:\n",
    "    best_accuracy_batch = accuracy_bs\n",
    "    best_batch_size = batch_s\n",
    "\n",
    "print(\n",
    "    f\"\\nBest batch size: {best_batch_size} with accuracy: {best_accuracy_batch*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97473327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test activations with the best batch size found\n",
    "activation_options = ['relu', 'sigmoid',\n",
    "                      'tanh', 'elu']  # relu (current) + 3 new\n",
    "best_activation = ''\n",
    "best_accuracy_activation = 0\n",
    "results_activation_tuning = []\n",
    "\n",
    "print(\n",
    "    f\"\\nTesting different activation functions with Batch Size: {best_batch_size}:\")\n",
    "for act_func in activation_options:\n",
    "  print(\n",
    "      f\"\\nTraining with Activation: {act_func}, Batch Size: {best_batch_size}...\")\n",
    "  model_act = create_cnn_model_for_step5(chosen_conv_config,\n",
    "                                         chosen_fc_config,\n",
    "                                         fixed_lr_arch,\n",
    "                                         act_func)\n",
    "  if act_func == activation_options[0]:  # Summary for first activation test\n",
    "    model_act.summary()\n",
    "\n",
    "  start_time_act_train = time.time()\n",
    "  history_act = model_act.fit(x_train_cnn, y_train_ann,\n",
    "                              epochs=fixed_epochs_arch,\n",
    "                              batch_size=best_batch_size,\n",
    "                              verbose=1,\n",
    "                              validation_data=(x_test_cnn, y_test_ann)\n",
    "                              )\n",
    "  end_time_act_train = time.time()\n",
    "  act_train_time = end_time_act_train - start_time_act_train\n",
    "\n",
    "  loss_act, accuracy_act = model_act.evaluate(\n",
    "      x_test_cnn, y_test_ann, verbose=0)\n",
    "\n",
    "  print(f\"Activation {act_func} - Test Accuracy: {accuracy_act*100:.2f}%\")\n",
    "  print(f\"Activation {act_func} - Training Time: {act_train_time:.2f} seconds\")\n",
    "\n",
    "  results_activation_tuning.append({\n",
    "      'activation': act_func,\n",
    "      'accuracy': accuracy_act,\n",
    "      'train_time': act_train_time\n",
    "  })\n",
    "\n",
    "  if accuracy_act > best_accuracy_activation:\n",
    "    best_accuracy_activation = accuracy_act\n",
    "    best_activation = act_func\n",
    "\n",
    "print(\n",
    "    f\"\\nBest activation function: '{best_activation}' with accuracy: {best_accuracy_activation*100:.2f}%\")\n",
    "\n",
    "# Finalized choices for this step\n",
    "final_batch_size = best_batch_size\n",
    "final_activation = best_activation\n",
    "print(\n",
    "    f\"Finalized for Step 5 - Batch Size: {final_batch_size}, Activation: {final_activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de96f73",
   "metadata": {},
   "source": [
    "## Step 6: Optimizers + Dropout\n",
    "- Test 2 new optimizers (e.g., Adam, RMSProp)\n",
    "- Add dropout layers in 2 positions\n",
    "- Try 2 rates (e.g., 0.25, 0.5)\n",
    "- Analyze regularization effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0becbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 6: Optimizers + Dropout ---\")\n",
    "\n",
    "# Use best settings from previous steps\n",
    "# chosen_conv_config, chosen_fc_config, fixed_epochs_arch, final_batch_size, final_activation are available.\n",
    "# fixed_lr_arch was for SGD. New optimizers will use their defaults or specified LRs.\n",
    "print(f\"Using: Arch-{chosen_conv_config}/{chosen_fc_config}, Epochs-{fixed_epochs_arch}, Batch-{final_batch_size}, Activation-{final_activation}\")\n",
    "\n",
    "results_optimizer_tuning = []\n",
    "best_optimizer_name = ''  # Will be 'sgd', 'adam', or 'rmsprop'\n",
    "best_optimizer_accuracy = \n",
    "\n",
    "def create_cnn_model_for_step6(conv_config, fc_config, activation, optimizer_name, dropout_configs=None):\n",
    "  \"\"\"\n",
    "  dropout_configs: list of tuples (layer_index_to_insert_after, dropout_rate)\n",
    "                   or specific layer types e.g., ('after_conv_pool', rate), ('after_fc', rate)\n",
    "                   For simplicity, let's define specific insertion points:\n",
    "                   dropout_configs = {'after_pool1': 0.0, 'after_fc1': 0.0} (example)\n",
    "  \"\"\"\n",
    "  model = Sequential()\n",
    "\n",
    "  # Conv Block 1\n",
    "  model.add(Conv2D(conv_config[0][0], conv_config[0][1],\n",
    "            activation=activation, input_shape=(28, 28, 1)))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  if dropout_configs and dropout_configs.get('after_pool1', 0.0) > 0:\n",
    "    model.add(Dropout(dropout_configs['after_pool1']))\n",
    "\n",
    "  # Subsequent Conv Layers (assuming chosen_conv_config might have more than 1)\n",
    "  for i in range(1, len(conv_config)):\n",
    "    model.add(Conv2D(conv_config[i][0],\n",
    "              conv_config[i][1], activation=activation))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # Potentially add more specific dropout points here if needed, e.g., 'after_pool2'\n",
    "    # For now, one dropout point after first pool, one after first FC for simplicity of \"2 positions\"\n",
    "\n",
    "  model.add(Flatten())\n",
    "\n",
    "  # FC Layers\n",
    "  if fc_config:  # If there are hidden FC layers\n",
    "    model.add(Dense(fc_config[0], activation=activation))\n",
    "    if dropout_configs and dropout_configs.get('after_fc1', 0.0) > 0:\n",
    "      model.add(Dropout(dropout_configs['after_fc1']))\n",
    "    # For multiple FC layers, this dropout is only after the first one.\n",
    "    # More complex dropout strategies could be implemented if needed.\n",
    "    for i in range(1, len(fc_config)):\n",
    "      model.add(Dense(fc_config[i], activation=activation))\n",
    "\n",
    "  model.add(Dense(10, activation='softmax'))  # Output layer\n",
    "\n",
    "  if optimizer_name == 'sgd':\n",
    "    optimizer = SGD(learning_rate=fixed_lr_arch)  # Use the tuned LR for SGD\n",
    "  elif optimizer_name == 'adam':\n",
    "    optimizer = Adam()  # Default LR for Adam\n",
    "  elif optimizer_name == 'rmsprop':\n",
    "    optimizer = RMSprop()  # Default LR for RMSprop\n",
    "  else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Test Optimizers (without dropout initially)\n",
    "optimizer_options = ['sgd', 'adam', 'rmsprop']\n",
    "print(\"\\nTesting different optimizers (without dropout):\")\n",
    "\n",
    "for opt_name in optimizer_options:\n",
    "  print(f\"\\nTraining with Optimizer: {opt_name}...\")\n",
    "  model_opt = create_cnn_model_for_step6(chosen_conv_config,\n",
    "                                         chosen_fc_config,\n",
    "                                         final_activation,\n",
    "                                         opt_name,\n",
    "                                         dropout_configs=None)  # No dropout yet\n",
    "  if opt_name == optimizer_options[0]:\n",
    "    model_opt.summary()  # Show summary for the first optimizer tested in this step\n",
    "\n",
    "  start_time_opt_train = time.time()\n",
    "  history_opt = model_opt.fit(x_train_cnn, y_train_ann,\n",
    "                              epochs=fixed_epochs_arch,\n",
    "                              batch_size=final_batch_size,\n",
    "                              verbose=1,\n",
    "                              validation_data=(x_test_cnn, y_test_ann)\n",
    "                              )\n",
    "  end_time_opt_train = time.time()\n",
    "  opt_train_time = end_time_opt_train - start_time_opt_train\n",
    "\n",
    "  loss_opt, accuracy_opt = model_opt.evaluate(\n",
    "      x_test_cnn, y_test_ann, verbose=0)\n",
    "\n",
    "  print(f\"Optimizer {opt_name} - Test Accuracy: {accuracy_opt*100:.2f}%\")\n",
    "  print(f\"Optimizer {opt_name} - Training Time: {opt_train_time:.2f} seconds\")\n",
    "\n",
    "  results_optimizer_tuning.append({\n",
    "      'optimizer': opt_name,\n",
    "      'accuracy': accuracy_opt,\n",
    "      'train_time': opt_train_time\n",
    "  })\n",
    "\n",
    "  if accuracy_opt > best_optimizer_accuracy:\n",
    "    best_optimizer_accuracy = accuracy_opt\n",
    "    best_optimizer_name = opt_name\n",
    "\n",
    "print(\n",
    "    f\"\\nBest optimizer (without dropout): '{best_optimizer_name}' with accuracy: {best_optimizer_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Test Dropout with the best optimizer\n",
    "print(f\"\\nTesting Dropout with Optimizer: {best_optimizer_name}\")\n",
    "\n",
    "dropout_rate_options = [0.25, 0.5]  # Try 2 rates < 0.85\n",
    "# Define 2 positions: e.g., after first max pooling, and after first (if any) hidden dense layer.\n",
    "# If no hidden FC layer in chosen_fc_config, dropout 'after_fc1' won't apply or needs adjustment.\n",
    "\n",
    "dropout_position_configs_to_test = []\n",
    "# Config 1: Dropout after Pool1 only\n",
    "dropout_position_configs_to_test.append(\n",
    "    # Placeholder for rates\n",
    "    ('Pool1_Only', {'after_pool1': 0.0, 'after_fc1': 0.0}))\n",
    "# Config 2: Dropout after FC1 only (if FC1 exists)\n",
    "if chosen_fc_config:  # Only if there is a hidden FC layer\n",
    "  dropout_position_configs_to_test.append(\n",
    "      ('FC1_Only', {'after_pool1': 0.0, 'after_fc1': 0.0}))\n",
    "# Config 3: Dropout at both positions (if FC1 exists)\n",
    "if chosen_fc_config:\n",
    "  dropout_position_configs_to_test.append(\n",
    "      ('Pool1_and_FC1', {'after_pool1': 0.0, 'after_fc1': 0.0}))\n",
    "\n",
    "if not chosen_fc_config and not any('FC1' in name for name, _ in dropout_position_configs_to_test):\n",
    "  print(\"No hidden FC layers in the chosen architecture. Dropout will only be tested after pooling.\")\n",
    "  # If chosen_fc_config is empty, FC1_Only and Pool1_and_FC1 might not be applicable in the current simple setup.\n",
    "  # We ensure at least one dropout strategy is tested. If only pool dropout is possible:\n",
    "  if not dropout_position_configs_to_test:\n",
    "    dropout_position_configs_to_test.append(\n",
    "        ('Pool1_Only', {'after_pool1': 0.0, 'after_fc1': 0.0}))\n",
    "\n",
    "results_dropout_tuning = []\n",
    "best_dropout_config_name = 'None'\n",
    "# Initialize with no-dropout accuracy\n",
    "best_dropout_accuracy = best_optimizer_accuracy\n",
    "final_dropout_config_for_model = None\n",
    "\n",
    "for rate in dropout_rate_options:\n",
    "  for config_name, base_config_template in dropout_position_configs_to_test:\n",
    "    current_dropout_config = {}\n",
    "    full_config_name = f\"{config_name}_Rate{rate}\"\n",
    "\n",
    "    if 'Pool1' in config_name:  # Apply rate to after_pool1 if this config name suggests it\n",
    "      current_dropout_config['after_pool1'] = rate\n",
    "    else:\n",
    "      # Default to no dropout if not specified for this position\n",
    "      current_dropout_config['after_pool1'] = 0.0\n",
    "\n",
    "    if 'FC1' in config_name and chosen_fc_config:  # Apply rate to after_fc1 if specified and FC layer exists\n",
    "      current_dropout_config['after_fc1'] = rate\n",
    "    else:\n",
    "      # Default to no dropout or if no FC layer\n",
    "      current_dropout_config['after_fc1'] = 0.0\n",
    "\n",
    "    # Skip if this combination results in no dropout, unless it's the baseline to compare against\n",
    "    if current_dropout_config.get('after_pool1', 0.0) == 0.0 and current_dropout_config.get('after_fc1', 0.0) == 0.0:\n",
    "      print(f\"Skipping {full_config_name} as it results in no dropout layers.\")\n",
    "      continue\n",
    "\n",
    "    print(\n",
    "        f\"\\nTraining with Dropout Config: {full_config_name}, Rate: {rate}, Positions: {current_dropout_config}\")\n",
    "    model_dropout = create_cnn_model_for_step6(chosen_conv_config,\n",
    "                                               chosen_fc_config,\n",
    "                                               final_activation,\n",
    "                                               best_optimizer_name,\n",
    "                                               dropout_configs=current_dropout_config)\n",
    "\n",
    "    # model_dropout.summary() # Optional: view summary for each dropout config\n",
    "\n",
    "    start_time_dropout_train = time.time()\n",
    "    history_dropout = model_dropout.fit(x_train_cnn, y_train_ann,\n",
    "                                        epochs=fixed_epochs_arch,\n",
    "                                        batch_size=final_batch_size,\n",
    "                                        verbose=1,\n",
    "                                        validation_data=(\n",
    "                                            x_test_cnn, y_test_ann)\n",
    "                                        )\n",
    "    end_time_dropout_train = time.time()\n",
    "    dropout_train_time = end_time_dropout_train - start_time_dropout_train\n",
    "\n",
    "    loss_dropout, accuracy_dropout = model_dropout.evaluate(\n",
    "        x_test_cnn, y_test_ann, verbose=0)\n",
    "\n",
    "    print(\n",
    "        f\"Dropout ({full_config_name}) - Test Accuracy: {accuracy_dropout*100:.2f}%\")\n",
    "    print(\n",
    "        f\"Dropout ({full_config_name}) - Training Time: {dropout_train_time:.2f} seconds\")\n",
    "\n",
    "    # Analyze regularization: Compare train/val accuracy if history_dropout.history has 'val_accuracy' and 'accuracy'\n",
    "    # For now, just logging test accuracy.\n",
    "\n",
    "    results_dropout_tuning.append({\n",
    "        'config_name': full_config_name,\n",
    "        'dropout_map': current_dropout_config,\n",
    "        'accuracy': accuracy_dropout,\n",
    "        'train_time': dropout_train_time\n",
    "    })\n",
    "\n",
    "    if accuracy_dropout > best_dropout_accuracy:\n",
    "      best_dropout_accuracy = accuracy_dropout\n",
    "      best_dropout_config_name = full_config_name\n",
    "      final_dropout_config_for_model = current_dropout_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\\nBest Dropout configuration: '{best_dropout_config_name}' with accuracy: {best_dropout_accuracy*100:.2f}%\")\n",
    "if final_dropout_config_for_model:\n",
    "  print(f\"Selected dropout parameters: {final_dropout_config_for_model}\")\n",
    "else:\n",
    "  print(\"No dropout configuration improved accuracy over the baseline optimizer.\")\n",
    "\n",
    "# Final choices for this step\n",
    "final_optimizer_name = best_optimizer_name\n",
    "# This will be None if no dropout was better\n",
    "final_dropout_settings = final_dropout_config_for_model\n",
    "\n",
    "print(\n",
    "    f\"Finalized for Step 6 - Optimizer: {final_optimizer_name}, Dropout Config: {final_dropout_settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3d480",
   "metadata": {},
   "source": [
    "## Step 7: Final Model & Report Writing\n",
    "- Choose best overall model\n",
    "- Document why it's best (final analysis)\n",
    "- Start report (Word format first)\n",
    "- Add graphs, stats, insights\n",
    "- Ensure all members can run the notebook independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa339a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 7: Final Model & Report Writing ---\")\n",
    "\n",
    "# Consolidate all best parameters found:\n",
    "final_model_conv_config = chosen_conv_config\n",
    "final_model_fc_config = chosen_fc_config\n",
    "final_model_epochs = fixed_epochs_arch\n",
    "final_model_lr_sgd = fixed_lr_arch  # Relevant if SGD is the final optimizer\n",
    "final_model_batch_size = final_batch_size\n",
    "final_model_activation = final_activation\n",
    "final_model_optimizer_name = final_optimizer_name\n",
    "final_model_dropout_config = final_dropout_settings  # This can be None\n",
    "\n",
    "print(\"\\n--- Final Model Configuration --- \")\n",
    "print(f\"Convolutional Layers Config: {final_model_conv_config}\")\n",
    "print(f\"Fully Connected Layers Config: {final_model_fc_config}\")\n",
    "print(f\"Epochs: {final_model_epochs}\")\n",
    "print(f\"Batch Size: {final_model_batch_size}\")\n",
    "print(f\"Activation Function: {final_model_activation}\")\n",
    "print(f\"Optimizer: {final_model_optimizer_name}\")\n",
    "if final_model_optimizer_name == 'sgd':\n",
    "  print(f\"  SGD Learning Rate: {final_model_lr_sgd}\")\n",
    "print(\n",
    "    f\"Dropout Configuration: {final_model_dropout_config if final_model_dropout_config else 'No Dropout'}\")\n",
    "\n",
    "# Build the final model using the function from Step 6 (or a dedicated one if needed)\n",
    "# create_cnn_model_for_step6 can be reused as it handles all these parameters.\n",
    "print(\"\\nBuilding and training the final model...\")\n",
    "final_model = create_cnn_model_for_step6(conv_config=final_model_conv_config,\n",
    "                                         fc_config=final_model_fc_config,\n",
    "                                         activation=final_model_activation,\n",
    "                                         optimizer_name=final_model_optimizer_name,\n",
    "                                         dropout_configs=final_model_dropout_config)\n",
    "\n",
    "final_model.summary()\n",
    "\n",
    "start_time_final_train = time.time()\n",
    "final_history = final_model.fit(x_train_cnn, y_train_ann,\n",
    "                                epochs=final_model_epochs,\n",
    "                                batch_size=final_model_batch_size,\n",
    "                                verbose=1,\n",
    "                                # Good to see validation performance\n",
    "                                validation_data=(x_test_cnn, y_test_ann)\n",
    "                                )\n",
    "end_time_final_train = time.time()\n",
    "final_train_time = end_time_final_train - start_time_final_train\n",
    "\n",
    "final_loss, final_accuracy = final_model.evaluate(\n",
    "    x_test_cnn, y_test_ann, verbose=0)\n",
    "\n",
    "print(\"\\n--- Final Model Performance --- \")\n",
    "print(f\"Test Accuracy: {final_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {final_loss:.4f}\")\n",
    "print(f\"Parameters: {final_model.count_params()}\")\n",
    "print(f\"Training Time: {final_train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Analysis (Summary of Choices) --- \")\n",
    "print(\"The final model configuration was determined through a step-by-step tuning process:\")\n",
    "print(f\"1. Baseline models (ANN & SVM) established initial performance benchmarks.\")\n",
    "print(\n",
    "    f\"2. Epoch Tuning (Step 2): A base CNN was trained for {epoch_options} epochs, and {best_epoch_cnn} epochs was chosen.\")\n",
    "print(\n",
    "    f\"3. Learning Rate Testing (Step 3): For SGD, LRs {learning_rate_options} were tested; {best_lr_cnn} was chosen.\")\n",
    "print(f\"4. Model Architecture Variation (Step 4): Various Conv/FC layer combinations were tested. \")\n",
    "print(\n",
    "    f\"   The chosen architecture ({best_overall_arch_config['name'] if best_overall_arch_config else 'Default/Base'}) aimed for high accuracy with reasonable parameters.\")\n",
    "print(\n",
    "    f\"5. Batch Sizes & Activations (Step 5): Batch sizes {batch_size_options} were tested, leading to {final_model_batch_size}. \")\n",
    "print(\n",
    "    f\"   Activations {activation_options} were tested, leading to '{final_model_activation}'.\")\n",
    "print(\n",
    "    f\"6. Optimizers & Dropout (Step 6): Optimizers {optimizer_options} were tested. '{final_model_optimizer_name}' was chosen.\")\n",
    "if final_model_dropout_config:\n",
    "  print(\n",
    "      f\"   Dropout was then applied ({final_model_dropout_config}), achieving the reported accuracy.\")\n",
    "else:\n",
    "  print(\"   Dropout was tested but did not yield improvement over the model without it for the chosen optimizer.\")\n",
    "print(\"This systematic approach helps in isolating the impact of each hyperparameter set.\")\n",
    "print(\"The final model represents the best configuration found through this specific sequence of tests.\")\n",
    "print(\"Further improvements could potentially be found with more extensive grid searches, different parameter ranges, or more advanced techniques.\")\n",
    "\n",
    "print(\"\\n--- End of MNIST Exeploration Project Implementation ---\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
